{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSHVHe9ZUBf",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **Project : A Case Study of ExpressWay Logistics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCRFIa5MZUBg",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Business Overview:**\n",
        "\n",
        "ExpressWay Logistics is a dynamic logistics service provider, committed to delivering efficient, reliable and cost-effective courier transportation and warehousing solutions. With a focus on speed, precision and customer satisfaction, we aim to be the go-to partner for our customers seeking seamless courier services. Our core service involves ensuring operational efficiency throughout our delivery and courier services, including inventory management, durable packaging and swift dispatch of couriers, real time tracking of shipments and on-time delivery of couriers as promised. We are committed to enhance our logistics and courier services and improve seamless connectivity for our customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO_aV3ecZUBh",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Current Challenge:**\n",
        "\n",
        "ExpressWay Logistics faces numerous challenges in ensuring seamless deliveries and customer satisfaction. These challenges include managing various customer demands simultaneously, addressing delays in deliveries and ensuring products arrive intact and safe. Additionally, the company struggles with complexity of efficiently storing and handling a large volume of packages and ultimately meeting customer expectations. Moreover, maintaining a skilled workforce capable of handling various aspects of logistics operations presents its own set of challenges. Overcoming these obstacles requires a comprehensive approach that integrates innovative technology, strategic planning, and continuous improvement initiatives to ensure smooth operations and exceptional service delivery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L11JPITnd9q"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "Our primary objective is to conduct a sentiment analysis of user-generated reviews across various digital channels and platforms. By paying attention to their feedback, we want to find ways to make our services better - like handling different customer demands simultaneously, dealing with late deliveries, and keeping packages secured and intact. Through the application of prompt engineering methodologies and sentiment analysis, we'll figure out if sentiments expressed by users for our courier services are Positive or Negative. This will help us understand where we need to improve in order to meet customer expectations and keep them happy. With a focus on getting better all the time, we'll overcome the challenges at ExpressWay Logistics and make our services the best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA-gDJc4ZUBh",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Data Description:**\n",
        "\n",
        "The dataset titled \"courier-service_reviews.csv\" is structured to facilitate sentiment analysis for courier service reviews. Here's a brief description of the data columns:\n",
        "\n",
        "1. id: This column contains unique identifiers for each review entry. It helps in distinguishing and referencing individual reviews.\n",
        "2. review: This column includes the actual text of the courier service reviews. The reviews are likely composed of customer opinions and experiences regarding different aspects of the services provided by ExpressWay Logistics.\n",
        "3. sentiment: This column provides an additional layer of classification (positive and negative) for the mentioned reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2QHxFF2ZUBi",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##**Step 1. Setup (2 Marks)**\n",
        "\n",
        "(A) Writing/Creating the config.json file  (2 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcewd3SmZUBi",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-adSo_JZUBi",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "dfe12584-bf5c-426c-ead5-b40de0417f45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m809.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.0 tiktoken datasets session-info --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDNABxMdZUBi",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Imports\n",
        "\n",
        "Import all Python packages required to access the Azure Open AI API and to access datasets and create examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OasyGdMKZUBj",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import all Python packages required to access the Azure Open AI API.\n",
        "# Import additional packages required to access datasets and create examples.\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import random\n",
        "import tiktoken\n",
        "import session_info\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwYdLZnCZUBj",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvXH0tKFnd9r"
      },
      "source": [
        "**(A) Writing/Creating the config.json file (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y3c-Aaynd9r"
      },
      "outputs": [],
      "source": [
        "# Define your configuration information\n",
        "config_data = {\n",
        "    \"AZURE_OPENAI_KEY\": \"\",\n",
        "    \"AZURE_OPENAI_ENDPOINT\": \"\",\n",
        "    \"AZURE_OPENAI_APITYPE\": \"\",\n",
        "    \"AZURE_OPENAI_APIVERSION\": \"\",\n",
        "    \"CHATGPT_MODEL\": \"\"\n",
        "}\n",
        "\n",
        "#Replace \"\" with your credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1MKyRrPnd9r",
        "outputId": "6bbcc7d9-2449-415c-e8e4-c94ee28c52f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config file created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Write the configuration information into the config.json file\n",
        "with open('config.json', 'w') as config_file:\n",
        "    json.dump(config_data, config_file, indent=4)\n",
        "\n",
        "print(\"Config file created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZFRplo_nd9s"
      },
      "source": [
        "Reading the config.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8quG4NzgZUBj",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "with open('config.json', 'r') as az_creds:\n",
        "    data = az_creds.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUoID4DfZUBj",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "creds = json.loads(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YplHdS7lZUBj",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "openai.api_key = creds[\"AZURE_OPENAI_KEY\"]\n",
        "openai.api_base = creds[\"AZURE_OPENAI_ENDPOINT\"]\n",
        "openai.api_type = creds[\"AZURE_OPENAI_APITYPE\"]\n",
        "openai.api_version = creds[\"AZURE_OPENAI_APIVERSION\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga7hDTIjZUBj",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "chat_model_id = creds[\"CHATGPT_MODEL\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gANLvw3ZUBj",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Utilities\n",
        "\n",
        "Define a function for token counter to keep track of the completion window available in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0eFci3SZUBj",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_messages(messages):\n",
        "\n",
        "    \"\"\"\n",
        "    Return the number of tokens used by a list of messages.\n",
        "    Adapted from the Open AI cookbook token counter\n",
        "    \"\"\"\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Each message is sandwiched with <|start|>role and <|end|>\n",
        "    # Hence, messages look like: <|start|>system or user or assistant{message}<|end|>\n",
        "\n",
        "    tokens_per_message = 3 # token1:<|start|>, token2:system(or user or assistant), token3:<|end|>\n",
        "\n",
        "    num_tokens = 0\n",
        "\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3tyrkxXZUBk",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Task: Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mixa5pEEZUBk",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##**Step 2: Assemble Data (4 Marks)**\n",
        "\n",
        "(A) Upload and Read csv File (2 Marks)\n",
        "\n",
        "(B) Creating a new column named as \"label\" (target column) corresponding to the sentiments in the dataset (2 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7JOH1F_Vc8B"
      },
      "source": [
        "Read the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1AkGAqQnd9s"
      },
      "source": [
        "**(A) Upload and read csv file (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYYzmBcBnd9s"
      },
      "outputs": [],
      "source": [
        "cs_reviews_df = \"_________\"\n",
        "# Read CSV File Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk5XzN0Bnd9t"
      },
      "source": [
        "**(B) Creating a new column named as \"label\" (target column) corresponding to the sentiments in the dataset (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAhcKIbdnd9t"
      },
      "outputs": [],
      "source": [
        "cs_reviews_df['label'] = \"__________\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxVoJw1rnd9t"
      },
      "source": [
        "Split the data into two segments (use split_ratio of 0.2) - one segment (80%) that gives us a pool to draw few-shot examples from and another segment (20%) that gives us a pool of gold examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq_MCC0knd9t"
      },
      "outputs": [],
      "source": [
        "cs_examples_df, cs_gold_examples_df = train_test_split(\n",
        "    cs_reviews_df, #<- the full dataset\n",
        "    test_size=0.2, #<- 20% random sample selected for gold examples\n",
        "    random_state=42 #<- ensures that the splits are the same for every session\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY0x4dfUnd9t"
      },
      "source": [
        "Select the correct columns for further analysis which should exclude the target column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa66SFM5nd9t"
      },
      "outputs": [],
      "source": [
        "columns_to_select = ['review','sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkgxygDDnd9t"
      },
      "source": [
        "Create gold examples and select a random sample (depends on the learner based on the session runtime - example:21) of rows from the gold examples dataframe(cs_gold_examples_df)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFQSROL3nd9t"
      },
      "outputs": [],
      "source": [
        "gold_examples = (\n",
        "        cs_gold_examples_df.loc[:, columns_to_select]\n",
        "                                     .sample(21, random_state=42) #<- ensures that gold examples are the same for every session\n",
        "                                     .to_json(orient='records')   # for better readability\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrz3Db4nd9t"
      },
      "source": [
        "To select gold examples for this session, sample randomly from the test data using a `random_state=42`. This ensures that the examples from multiple runs of the sampling are the same (i.e., they are randomly selected but do not change between different runs of the notebook). Note that we are doing this only to keep execution times low for illustration. In practise, large number of gold examples facilitate robust estimates of model accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVI7gT94ZUBk",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##**Step 3: Derive Prompt (14 Marks)**\n",
        "\n",
        "(A) Write Zero Shot Prompt (5 Marks)\n",
        "\n",
        "(B) Write Few Shot Prompt (5 Marks)\n",
        "\n",
        "(C) Print Create Examples (2 Marks)\n",
        "\n",
        "(D) Print Few shot Prompt (2 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZGV2plcZUBk",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gxodRdrnd9t"
      },
      "outputs": [],
      "source": [
        "user_message_template = \"\"\"```{courier_service_review}```\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUjwO1rzZUBk",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(A) Write Zero Shot Prompt (5 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgmjUfaYZUBk",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "zero_shot_system_message = \"\"\"__________\"\"\"\n",
        "#  Write Zero Shot Prompt Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md8c677uaPsE"
      },
      "outputs": [],
      "source": [
        "zero_shot_prompt = [{'role':'system', 'content': zero_shot_system_message}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ypjy7rZUBk",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(B) Write Few Shot Prompt (5 Marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBOPcb2snd9u"
      },
      "source": [
        "For the few-shot prompt, there is no change in the system message compared with the zero-shot prompt. However, we augment this system message with few shot examples.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZZdkwGTZUBk",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "few_shot_system_message = \"\"\"__________\"\"\"\n",
        "#  Write Few Shot Prompt Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z08c4Ytond9v"
      },
      "source": [
        "Merely selecting random samples from the polarity subsets is not enough because the examples included in a prompt are prone to a set of known biases such as:\n",
        " - Majority label bias (frequent answers in predictions)\n",
        " - Recency bias (examples near the end of the prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjWV1ghqnd9v"
      },
      "source": [
        "To avoid these biases, it is important to have a balanced set of examples that are arranged in random order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKRWG4sGad2f"
      },
      "source": [
        "Let us now look at how we can assemble examples to go along with this few-shot system message and compose a few-shot prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLSPqPobbOOn"
      },
      "source": [
        "###Define \"create_examples\" function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-95O-bqGnd9v"
      },
      "outputs": [],
      "source": [
        "def create_examples(dataset, n=4):\n",
        "\n",
        "    \"\"\"\n",
        "    Return a JSON list of randomized examples of size 2n with two classes.\n",
        "    Create subsets of each class, choose random samples from the subsets,\n",
        "    merge and randomize the order of samples in the merged list.\n",
        "    Each run of this function creates a different random sample of examples\n",
        "    chosen from the training data.\n",
        "\n",
        "    Args:\n",
        "        dataset (DataFrame): A DataFrame with examples (review + label)\n",
        "        n (int): number of examples of each class to be selected\n",
        "\n",
        "    Output:\n",
        "        randomized_examples (JSON): A JSON with examples in random order\n",
        "    \"\"\"\n",
        "\n",
        "    positive_reviews = (dataset.sentiment == 'Positive')\n",
        "    negative_reviews = (dataset.sentiment == 'Negative')\n",
        "    columns_to_select = ['review', 'sentiment']\n",
        "\n",
        "    positive_examples = dataset.loc[positive_reviews, columns_to_select].sample(n)\n",
        "    negative_examples = dataset.loc[negative_reviews, columns_to_select].sample(n)\n",
        "\n",
        "    examples = pd.concat([positive_examples, negative_examples])\n",
        "    # sampling without replacement is equivalent to random shuffling\n",
        "    randomized_examples = examples.sample(2*n, replace=False)\n",
        "\n",
        "    return randomized_examples.to_json(orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtWb-G5Cnd9v"
      },
      "source": [
        "Use the above create_examples function to create examples for few shot prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1Pv5t4und9v"
      },
      "outputs": [],
      "source": [
        "examples = create_examples(cs_examples_df, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd3yfkmMnd9v"
      },
      "source": [
        "**(C) Print Created Examples (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdI7BJ1Xnd9v"
      },
      "outputs": [],
      "source": [
        "\"__________\"\n",
        "# Print the created examples here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw6O7K7pbX-V"
      },
      "source": [
        "2. Define \"create_prompt\" function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCNKfvb7ayDP"
      },
      "outputs": [],
      "source": [
        "def create_prompt(system_message, examples, user_message_template):\n",
        "\n",
        "    \"\"\"\n",
        "    Return a prompt message in the format expected by the Open AI API.\n",
        "    Loop through the examples and parse them as user message and assistant\n",
        "    message.\n",
        "\n",
        "    Args:\n",
        "        system_message (str): system message with instructions for sentiment analysis\n",
        "        examples (str): JSON string with list of examples\n",
        "        user_message_template (str): string with a placeholder for courier service reviews\n",
        "\n",
        "    Output:\n",
        "        few_shot_prompt (List): A list of dictionaries in the Open AI prompt format\n",
        "    \"\"\"\n",
        "\n",
        "    few_shot_prompt = [{'role':'system', 'content': system_message}]\n",
        "\n",
        "    for example in json.loads(examples):\n",
        "        example_review = example['review']\n",
        "        example_sentiment = example['sentiment']\n",
        "\n",
        "        few_shot_prompt.append(\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_message_template.format(\n",
        "                    courier_service_review=example_review\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "        few_shot_prompt.append(\n",
        "            {'role': 'assistant', 'content': f\"{example_sentiment}\"}\n",
        "        )\n",
        "\n",
        "    return few_shot_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAVcEeVand9w"
      },
      "source": [
        "Use the above create_prompt function to create few_shot_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJtKZumbnd9w"
      },
      "outputs": [],
      "source": [
        "few_shot_prompt = create_prompt(\n",
        "    few_shot_system_message,\n",
        "    examples,\n",
        "    user_message_template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Pp5gWQnd9w"
      },
      "source": [
        "**(D) Print the Few Shot Prompt (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EDt5Moind9w"
      },
      "outputs": [],
      "source": [
        "\"__________\"\n",
        "# Print the created few shot prompt here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHhvs9gnZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##**Step 4: Evaluate prompts (10 Marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RgX6kBjUrRZ"
      },
      "source": [
        "(A) Evaluate Zero Shot Prompt (3 Marks)\n",
        "\n",
        "(B) Evaluate Few Shot Prompt (3 marks)\n",
        "\n",
        "(C) Calculate Mean and Standard Deviation for Zero Shot and Few Shot (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU7BHj7xZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Define Evaluation scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek3GxuNYnd9w"
      },
      "outputs": [],
      "source": [
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "\n",
        "    \"\"\"\n",
        "    Return the micro-F1 score for predictions on gold examples.\n",
        "    For each example, we make a prediction using the prompt. Gold labels and\n",
        "    model predictions are aggregated into lists and compared to compute the\n",
        "    F1 score.\n",
        "\n",
        "    Args:\n",
        "        prompt (List): list of messages in the Open AI prompt format\n",
        "        gold_examples (str): JSON string with list of gold examples\n",
        "        user_message_template (str): string with a placeholder for courier service review\n",
        "\n",
        "    Output:\n",
        "        micro_f1_score (float): Micro-F1 score computed by comparing model predictions\n",
        "                                with ground truth\n",
        "    \"\"\"\n",
        "\n",
        "    model_predictions, ground_truths, review_texts = [], [], []\n",
        "\n",
        "    for example in json.loads(gold_examples):\n",
        "        gold_input = example['review']\n",
        "        user_input = [\n",
        "            {\n",
        "                'role':'user',\n",
        "                'content': user_message_template.format(courier_service_review=gold_input)\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                deployment_id=chat_model_id,\n",
        "                messages=prompt+user_input,\n",
        "                temperature=0, # <- Note the low temperature(For a deterministic response)\n",
        "                max_tokens=2 # <- Note how we restrict the output to not more than 2 tokens\n",
        "            )\n",
        "\n",
        "            prediction = response['choices'][0]['message']['content']\n",
        "            model_predictions.append(prediction.strip()) # <- removes extraneous white spaces\n",
        "            ground_truths.append(example['sentiment'])\n",
        "            review_texts.append(gold_input)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "\n",
        "    table_data = [[text, pred, truth] for text, pred, truth in zip(review_texts, model_predictions, ground_truths)]\n",
        "    headers = [\"Review\", \"Model Prediction\", \"Ground Truth\"]\n",
        "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "    return micro_f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmEiVPIZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(A) Evaluate zero shot prompt (3 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kar164F4nd9x"
      },
      "outputs": [],
      "source": [
        "\"_________\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya73XdatZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(B) Evaluate few shot prompt (3 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqQTgKUkZUBl",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\"_________\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbAllJYHnd9x"
      },
      "source": [
        " However, this is just *one* choice of examples. We will need to run these evaluations with multiple choices of examples to get a sense of variability in F1 score for the few-shot prompt. As an example, let us run evaluations for the few-shot prompt 5 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vp62FqPnd9x"
      },
      "outputs": [],
      "source": [
        "num_eval_runs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHK0X_0Lnd9x"
      },
      "outputs": [],
      "source": [
        "zero_shot_performance = []\n",
        "few_shot_performance = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPxO3P5Mnd9x"
      },
      "outputs": [],
      "source": [
        "for _ in tqdm(range(num_eval_runs)):\n",
        "\n",
        "    # For each run create a new sample of examples\n",
        "    examples = create_examples(cs_examples_df)\n",
        "\n",
        "    # Assemble the zero shot prompt with these examples\n",
        "    zero_shot_prompt = [{'role':'system', 'content': zero_shot_system_message}]\n",
        "\n",
        "    # Assemble the few shot prompt with these examples\n",
        "    few_shot_prompt = create_prompt(few_shot_system_message, examples, user_message_template)\n",
        "\n",
        "    # Evaluate zero shot prompt accuracy on gold examples\n",
        "    zero_shot_micro_f1 = evaluate_prompt(zero_shot_prompt, gold_examples, user_message_template)\n",
        "\n",
        "    # Evaluate few shot prompt accuracy on gold examples\n",
        "    few_shot_micro_f1 = evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)\n",
        "\n",
        "    zero_shot_performance.append(zero_shot_micro_f1)\n",
        "    few_shot_performance.append(few_shot_micro_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSJzAeZZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(C) Calculate Mean and Standard Deviation for Zero Shot and Few Shot (4 Marks)**\n",
        "\n",
        "Compute the average (mean) and measure the variability (standard deviation) of the evaluation scores for both zero shot and few shot prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlx7hRekacH2"
      },
      "outputs": [],
      "source": [
        "\"__________\"\n",
        "# Calculate for Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUt85F--aehR"
      },
      "outputs": [],
      "source": [
        "\"__________\"\n",
        "# Calculate for Few Shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdKlgVP1YPTR"
      },
      "source": [
        "##**Step 5: Observation and Insights and Business perspective (10 Marks)**\n",
        "\n",
        "Based on the projects, learner needs to share observations, learnings, insights and the business use case where these learnings can be beneficial.\n",
        "Provide a breakdown of the percentage of positive and negative reviews. Additionally, explain how this classification can assist ExpressWay Logistics in addressing the issues identified.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49fAgk2KYsZ9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PFtYALJZUBl",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iknx8Qu3ZUBl",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZoufdWnZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**----------------------------------------------------------------------------End-----------------------------------------------------------------------------------------**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}